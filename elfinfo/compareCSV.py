#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Compare two ELF section CSVs (generated by elfinfo.py) and produce a wide diff CSV.

Input CSV expected columns:
  - relative_path
  - filename
  - section_name
  - section_size (integer)

Rules:
- Group name defaults to section_name with the first leading '.' removed and uppercased.
- Also supports predefined "combined" groups (wildcards supported via fnmatch).
- If --all-files is omitted, only files present in BOTH old and new sets are compared.
- Otherwise, include Added (only in new) and Removed (only in old) with zeros on the missing side.

Output CSV columns:
  relative_dir, filename, status,
  <GROUP>_old, <GROUP>_new, <GROUP>_diff, <GROUP>_diff%, ... for all groups detected.
Rows are sorted by |<files-sort-by>_diff| descending (default FILESIZE).

-Top-N:
- Compute total diff per group across all files; pick Top-N groups by |diff| (`--top-n-groups`).
- For each Top group, list Top-N files by |diff| (`--top-n-files`).
- Deprecated: `--top-n` applies to both when specific values are not provided.
- Save Top-N to two files: top-n groups and top-n files (see --topn-groups-csv and --topn-files-csv).
- Top-N groups ordering can be controlled via --group-sort-by and --group-sort-metric.


Notes on diff%:
- diff% = (new - old) / old * 100
- If old == 0:
    - If new == 0 -> 0.0
    - Else -> sign(new-old) * 100.0 (finite cap, not inf), to keep it readable

Settings in --config-file (JSON) are optional and merged; CLI flags explicitly provided take precedence.

All code and comments are in English by request.
"""

import argparse
import csv
import json
import os
import sys
from collections import defaultdict
from fnmatch import fnmatch
from typing import Dict, List, Tuple, Set

# Detect whether a specific CLI option was explicitly provided
def _cli_has_option(name: str) -> bool:
    # Matches forms like: --opt, --opt=value, --opt value
    argv = sys.argv[1:]
    if any(a == name or a.startswith(name + "=") for a in argv):
        return True
    # Handle "--opt value" form
    for i, a in enumerate(argv[:-1]):
        if a == name:
            return True
    return False

# Load optional config JSON. Returns a dict; missing keys are fine.
def load_config(path: str) -> Dict:
    if not path:
        return {}
    if not os.path.isfile(path):
        return {}
    with open(path, "r", encoding="utf-8") as f:
        try:
            cfg = json.load(f)
            if not isinstance(cfg, dict):
                return {}
            return cfg
        except Exception:
            return {}

# ---------- Default Combined Group Definitions ----------
# Wildcards are supported (fnmatch). Keys are group names (UPPERCASE).
# FILESIZE is included by default per earlier spec (section_name == "FILESIZE").
DEFAULT_GROUP_DEFS: Dict[str, List[str]] = {
    "FILESIZE": ["FILESIZE"],
    "TEXT": [".text", ".text.*", ".init.text", ".fini.text"],
    "RODATA": [".rodata", ".rodata.*"],
    "DATA": [".data", ".data.*", ".sdata", ".sdata.*"],
    "BSS": [".bss", ".bss.*", ".sbss", ".sbss.*"],
    "REL": [".rel.*", ".rela.*"],
    "SYMTAB": [".symtab"],
    "STRTAB": [".strtab"],
    "DWARF": [".debug*", ".zdebug*"],
}

# Accept either 'relative_path' (old) or 'base_rel_dir' (new from elfinfo.py)
REQUIRED_ANY_REL = {"relative_path", "base_rel_dir"}
REQUIRED_COLUMNS_OTHERS = {"filename", "section_name", "section_size"}


def parse_args():
    p = argparse.ArgumentParser(description="Compare two ELF section CSVs and output a diff CSV.")
    p.add_argument("--old-csv", required=True, help="Path to old CSV (from elfinfo.py)")
    p.add_argument("--new-csv", required=True, help="Path to new CSV (from elfinfo.py)")
    p.add_argument("--files-csv", required=False, default="", help="Path to write the diff (files) CSV (optional, default: '<prefix>_files_<all|common>.csv')")
    p.add_argument("--out-dir", default="", help="Directory to write output files; defaults to '<oldbasename>_vs_<newbasename>'")
    p.add_argument("--all-files", action="store_true",
                   help="Include Added/Removed files; otherwise compare only common files")
    # New options replacing --top-n
    p.add_argument("--top-n-groups", type=int, default=0,
                   help="If >0, include Top N groups by |diff| in the summary")
    p.add_argument("--top-n-files", type=int, default=0,
                   help="If >0, for each Top group include Top N files by |diff| in the summary")
    # Backward-compat (deprecated): if provided, it will set both groups/files unless those are explicitly set
    p.add_argument("--top-n", type=int, default=None,
                   help="[Deprecated] If provided, applies to both --top-n-groups and --top-n-files unless those are explicitly set")
    p.add_argument("--topn-groups-csv", default="",
                   help="CSV to store Top-N groups only (no per-file rows)")
    p.add_argument("--topn-files-csv", default="",
                   help="CSV to store Top-N files per group (includes per-file rows with their group)")
    # Deprecated:
    p.add_argument("--groups-csv", default="",
                   help="Optional CSV to store the Top-N groups summary")
    p.add_argument("--config-file", default="",
                   help="Optional JSON file to override/add combined group definitions")
    p.add_argument("--files-sort-by", default="FILESIZE",
                   help="For files CSV: group name to sort rows by |diff| descending (default: FILESIZE)")
    p.add_argument("--files-sort-metric", default="absdiff",
                   choices=["absdiff", "diff", "old", "new", "diff%"],
                   help="For files CSV: metric used with --files-sort-by (default: absdiff)")
    p.add_argument("--group-sort-by", default="metric", choices=["metric", "name"],
                help="How to sort groups in Top-N outputs: 'metric' (by chosen metric) or 'name' (alphabetical). Default: metric")
    p.add_argument("--group-sort-metric", default="absdiff",
                choices=["absdiff", "diff", "old", "new", "diff%"],
                help="Metric used when --group-sort-by=metric for Top-N groups (default: absdiff)")

    p.add_argument("--output-prefix", default="", help="Prefix for output files; if not provided, defaults to '<oldbasename>_vs_<newbasename>'")
    return p.parse_args()


def load_group_defs(path: str) -> Dict[str, List[str]]:
    # Backward compatibility: keep reading only groups from the JSON file if provided.
    if path and os.path.isfile(path):
        with open(path, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except Exception:
                data = {}
        user_defs = data.get("groups", data) if isinstance(data, dict) else {}
        merged = {k.upper(): (v if isinstance(v, list) else [v]) for k, v in DEFAULT_GROUP_DEFS.items()}
        if isinstance(user_defs, dict):
            for k, v in user_defs.items():
                merged[k.upper()] = v if isinstance(v, list) else [v]
        return merged
    return {k.upper(): v[:] for k, v in DEFAULT_GROUP_DEFS.items()}


def read_elf_csv(path: str) -> List[Dict]:
    rows: List[Dict] = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        # Validate required columns flexibly: allow either 'relative_path' or 'base_rel_dir'
        fieldnames = set(reader.fieldnames or [])
        if not (REQUIRED_ANY_REL & fieldnames):
            raise ValueError(f"{path} is missing a relative path column; expected one of: {sorted(REQUIRED_ANY_REL)}")
        missing = REQUIRED_COLUMNS_OTHERS - fieldnames
        if missing:
            raise ValueError(f"{path} is missing required columns: {sorted(missing)}")
        for r in reader:
            try:
                size = int(r["section_size"]) if r["section_size"] != "" else 0
            except Exception:
                size = 0
            rel = r.get("relative_path", "")
            if rel == "":
                rel = r.get("base_rel_dir", "")
            rows.append({
                "relative_path": rel,
                "filename": r["filename"],
                "section_name": r["section_name"],
                "section_size": size,
            })
    return rows


def default_group_name(section_name: str) -> str:
    # Derive group from section name: remove first leading '.' and uppercase the head token.
    # Examples:
    #   ".text" -> "TEXT"
    #   ".rodata.str1.1" -> "RODATA"
    #   "FILESIZE" -> "FILESIZE"
    if not section_name:
        return "NULL"
    if section_name == "FILESIZE":
        return "FILESIZE"
    s = section_name
    if s.startswith("."):
        s = s[1:]
    head = s.split(".", 1)[0] if s else ""
    return head.upper() if head else "NULL"


def build_group_resolver(group_defs: Dict[str, List[str]]):
    """
    Return a function that maps (section_name) -> set of group names it belongs to.
    A section can belong to multiple combined groups; also include its default group.
    """
    patterns_by_group = {g.upper(): [p for p in patterns] for g, patterns in group_defs.items()}

    def resolve(section_name: str) -> Set[str]:
        out: Set[str] = set()
        for g, patterns in patterns_by_group.items():
            for pat in patterns:
                if fnmatch(section_name, pat):
                    out.add(g)
                    break
        out.add(default_group_name(section_name))
        return out

    return resolve


def aggregate_by_file_and_group(rows: List[Dict], resolve_groups) -> Dict[Tuple[str, str], Dict[str, int]]:
    """
    Return: {(relative_path, filename): {group: total_size}}
    """
    agg: Dict[Tuple[str, str], Dict[str, int]] = defaultdict(lambda: defaultdict(int))
    for r in rows:
        key = (r["relative_path"], r["filename"])
        secname = r["section_name"]
        size = r["section_size"]
        groups = resolve_groups(secname)
        for g in groups:
            agg[key][g] += size
    return agg


def safe_diff_pct(old: int, new: int) -> float:
    diff = new - old
    if old == 0:
        if new == 0:
            return 0.0
        return 100.0 if diff > 0 else -100.0
    return (diff / old) * 100.0


def format_float(x: float) -> str:
    return f"{x:.2f}"

# Human-readable 1024-based formatter (e.g., 1536 -> "1.50K", 1048576 -> "1.00M")
def humanize_1024(n: int) -> str:
    sign = "-" if n < 0 else ""
    a = abs(n)
    if a >= 1024 * 1024:
        return f"{sign}{a / (1024*1024):.2f}M"
    if a >= 1024:
        return f"{sign}{a / 1024:.2f}K"
    return f"{n}"


def compute_joined_keys(old_map: Dict, new_map: Dict, all_files: bool) -> List[Tuple[str, str]]:
    old_keys = set(old_map.keys())
    new_keys = set(new_map.keys())
    keys = sorted(old_keys | new_keys) if all_files else sorted(old_keys & new_keys)
    return keys


def write_diff_csv(path: str,
                   keys: List[Tuple[str, str]],
                   old_map: Dict[Tuple[str, str], Dict[str, int]],
                   new_map: Dict[Tuple[str, str], Dict[str, int]],
                   sort_by_group: str = "FILESIZE",
                   sort_metric: str = "absdiff") -> List[str]:
    """
    Write the wide diff CSV and return the ordered group list used in the header.
    The third column is `status` (Common/Added/Removed).
    """
    # Determine all groups across compared keys (union)
    seen = set()
    for k in keys:
        for src in (old_map.get(k, {}), new_map.get(k, {})):
            for g in src.keys():
                seen.add(g)

    ordered_groups: List[str] = []
    if "FILESIZE" in seen:
        ordered_groups.append("FILESIZE")
    ordered_groups.extend(sorted([g for g in seen if g != "FILESIZE"]))

    # Header with status as 3rd column
    header = ["relative_dir", "filename", "status"]
    for g in ordered_groups:
        header.extend([f"{g}_old", f"{g}_new", f"{g}_diff", f"{g}_diff%"])

    # Sort keys by metric for the requested group (default FILESIZE), descending
    sort_group = (sort_by_group or "FILESIZE").upper()
    metric = (sort_metric or "absdiff").lower()
    if sort_group in ordered_groups:
        def _metric_value(k: Tuple[str, str]):
            om = old_map.get(k, {})
            nm = new_map.get(k, {})
            oldv = om.get(sort_group, 0)
            newv = nm.get(sort_group, 0)
            diff = newv - oldv
            if metric == "diff":
                return diff
            elif metric == "absdiff":
                return abs(diff)
            elif metric == "old":
                return oldv
            elif metric == "new":
                return newv
            elif metric == "diff%":
                return safe_diff_pct(oldv, newv)
            return abs(diff)
        keys = sorted(keys, key=lambda k: (_metric_value(k), k[0], k[1]), reverse=True)
    # else: keep original order

    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(header)
        for rel, name in keys:
            om = old_map.get((rel, name), {})
            nm = new_map.get((rel, name), {})

            if om and nm:
                status = "Common"
            elif om and not nm:
                status = "Removed"
            else:
                status = "Added"

            # rel is the relative directory, now mapped to "relative_dir" column.
            row = [rel, name, status]
            for g in ordered_groups:
                oldv = om.get(g, 0)
                newv = nm.get(g, 0)
                diff = newv - oldv
                pct = safe_diff_pct(oldv, newv)
                row.extend([oldv, newv, diff, format_float(pct)])
            w.writerow(row)

    return ordered_groups


def compute_topn(ordered_groups: List[str],
                 keys: List[Tuple[str, str]],
                 old_map: Dict[Tuple[str, str], Dict[str, int]],
                 new_map: Dict[Tuple[str, str], Dict[str, int]],
                 n_groups: int,
                 n_files: int,
                 group_sort_by: str = "metric",
                 group_sort_metric: str = "absdiff"):
    """
    Returns:
      top_groups: list[(group, total_diff, total_abs_diff)]  # length <= n_groups
      top_files_by_group: dict[group] -> list[(key, old, new, diff, diff%)]  # each length <= n_files
      total_old: dict[group] -> int, total_new: dict[group] -> int
    """
    total_old: Dict[str, int] = defaultdict(int)
    total_new: Dict[str, int] = defaultdict(int)
    totals_diff: Dict[str, int] = defaultdict(int)
    for k in keys:
        om = old_map.get(k, {})
        nm = new_map.get(k, {})
        for g in ordered_groups:
            o = om.get(g, 0)
            n = nm.get(g, 0)
            total_old[g] += o
            total_new[g] += n
            totals_diff[g] += (n - o)

    # Build list of groups and sort per settings
    def _group_metric_value(g: str) -> float:
        m = (group_sort_metric or "absdiff").lower()
        if m == "diff":
            return float(totals_diff[g])
        elif m == "absdiff":
            return float(abs(totals_diff[g]))
        elif m == "old":
            return float(total_old[g])
        elif m == "new":
            return float(total_new[g])
        elif m == "diff%":
            return safe_diff_pct(total_old[g], total_new[g])
        return float(abs(totals_diff[g]))

    groups_list = list(ordered_groups)
    if group_sort_by == "name":
        groups_list.sort(key=lambda x: x)
    else:
        groups_list.sort(key=lambda x: (_group_metric_value(x), x), reverse=True)

    if n_groups:
        groups_list = groups_list[:n_groups]

    top_groups = [(g, totals_diff[g], abs(totals_diff[g])) for g in groups_list]

    top_files_by_group: Dict[str, List] = {}
    limit_files = n_files if n_files else None
    for g, diff, _ in top_groups:
        per_file = []
        for k in keys:
            rel, name = k
            om = old_map.get(k, {})
            nm = new_map.get(k, {})
            oldv = om.get(g, 0)
            newv = nm.get(g, 0)
            d = newv - oldv
            if d != 0:
                per_file.append((k, oldv, newv, d, safe_diff_pct(oldv, newv)))
        per_file_sorted = sorted(per_file, key=lambda t: abs(t[3]), reverse=True)
        if limit_files is not None:
            per_file_sorted = per_file_sorted[:limit_files]
        top_files_by_group[g] = per_file_sorted

    return top_groups, top_files_by_group, total_old, total_new



# --- Write Top-N Groups CSV ---
def write_topn_groups_csv(path: str,
                          top_groups,
                          total_old: Dict[str, int],
                          total_new: Dict[str, int]):
    """
    Columns: group, total_old, total_new, total_diff, total_diff%
    """
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["group", "total_old", "total_new", "total_diff", "total_diff%"])
        for g, gdiff, _ in top_groups:
            told = total_old.get(g, 0)
            tnew = total_new.get(g, 0)
            pct = safe_diff_pct(told, tnew)
            w.writerow([g, told, tnew, gdiff, format_float(pct)])


def write_topn_files_csv(path: str,
                          top_groups,
                          top_files_by_group,
                          old_map,
                          new_map):
    """
    Columns: group, file_relative_dir, file_name, status, old, new, diff, diff%
    """
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["group", "file_relative_dir", "file_name", "status", "old", "new", "diff", "diff%"])
        for g, _gdiff, _abs in top_groups:
            files = top_files_by_group.get(g, [])
            for (rel, name), oldv, newv, d, pct in files:
                omf = old_map.get((rel, name), {})
                nmf = new_map.get((rel, name), {})
                if omf and nmf:
                    status = "Common"
                elif omf and not nmf:
                    status = "Removed"
                else:
                    status = "Added"
                w.writerow([g, rel, name, status, oldv, newv, d, format_float(pct)])


def write_topfiles_used(path: str,
                        top_groups,
                        top_files_by_group):
    seen = set()
    lines = []
    for g, _gdiff, _abs in top_groups:
        for (rel, name), _oldv, _newv, _d, _pct in top_files_by_group.get(g, []):
            key = (rel, name)
            if key in seen:
                continue
            seen.add(key)
            if rel:
                lines.append(f"{rel}/{name}")
            else:
                lines.append(name)
    with open(path, "w", encoding="utf-8") as f:
        for line in lines:
            f.write(line + "\n")


def main():
    args = parse_args()
    cfg = load_config(args.config_file)

    # Deprecated flag handling
    if getattr(args, "groups_csv", ""):
        print("[WARN] --groups-csv is deprecated and ignored. Use --topn-groups-csv and --topn-files-csv instead.")

    # Groups: prefer config['groups'] if available, else fall back to file-as-groups or defaults
    if isinstance(cfg.get("groups"), dict):
        # Merge defaults with config groups
        group_defs = {k.upper(): v[:] for k, v in DEFAULT_GROUP_DEFS.items()}
        for k, v in cfg["groups"].items():
            group_defs[k.upper()] = v if isinstance(v, list) else [v]
    else:
        group_defs = load_group_defs(args.config_file)

    # Compute effective output prefix
    if args.output_prefix:
        effective_output_prefix = args.output_prefix
    else:
        oldb = os.path.splitext(os.path.basename(args.old_csv))[0]
        newb = os.path.splitext(os.path.basename(args.new_csv))[0]
        effective_output_prefix = f"{oldb}_vs_{newb}"

    # Compute effective output directory
    if args.out_dir:
        effective_out_dir = args.out_dir
    else:
        effective_out_dir = effective_output_prefix
    os.makedirs(effective_out_dir, exist_ok=True)

    # Compute effective settings with CLI precedence (only if explicitly provided)
    if _cli_has_option("--all-files"):
        effective_all_files = args.all_files
    else:
        effective_all_files = bool(cfg.get("all_files", args.all_files))

    # Determine Top-N (groups/files) with CLI precedence and legacy support
    # CLI explicit checks
    cli_has_tng = _cli_has_option("--top-n-groups")
    cli_has_tnf = _cli_has_option("--top-n-files")
    cli_has_tn  = _cli_has_option("--top-n")

    # Base values from config (prefer new keys; fall back to legacy 'top_n')
    cfg_tng = cfg.get("top_n_groups")
    cfg_tnf = cfg.get("top_n_files")
    cfg_tn  = cfg.get("top_n")

    # Start from argparse defaults
    effective_top_n_groups = args.top_n_groups
    effective_top_n_files  = args.top_n_files

    if cli_has_tng:
        effective_top_n_groups = args.top_n_groups
    elif cfg_tng is not None:
        effective_top_n_groups = int(cfg_tng)
    elif cli_has_tn and args.top_n is not None:
        effective_top_n_groups = int(args.top_n)
    elif cfg_tn is not None:
        effective_top_n_groups = int(cfg_tn)

    if cli_has_tnf:
        effective_top_n_files = args.top_n_files
    elif cfg_tnf is not None:
        effective_top_n_files = int(cfg_tnf)
    elif cli_has_tn and args.top_n is not None:
        effective_top_n_files = int(args.top_n)
    elif cfg_tn is not None:
        effective_top_n_files = int(cfg_tn)


    # Files sorting options (CLI > config > defaults)
    if _cli_has_option("--files-sort-by"):
        effective_files_sort_by = args.files_sort_by
    else:
        effective_files_sort_by = cfg.get("files_sort_by", args.files_sort_by)

    if _cli_has_option("--files-sort-metric"):
        effective_files_sort_metric = args.files_sort_metric
    else:
        effective_files_sort_metric = cfg.get("files_sort_metric", args.files_sort_metric)

    # Group sorting options for Top-N outputs
    if _cli_has_option("--group-sort-by"):
        effective_group_sort_by = args.group_sort_by
    else:
        effective_group_sort_by = cfg.get("group_sort_by", args.group_sort_by)

    if _cli_has_option("--group-sort-metric"):
        effective_group_sort_metric = args.group_sort_metric
    else:
        effective_group_sort_metric = cfg.get("group_sort_metric", args.group_sort_metric)

    if _cli_has_option("--files-csv"):
        effective_files_csv = args.files_csv
    elif "files_csv" in cfg:
        effective_files_csv = cfg.get("files_csv", args.files_csv)
    else:
        suffix = "_files_all.csv" if effective_all_files else "_files_common.csv"
        effective_files_csv = os.path.join(effective_out_dir, effective_output_prefix + suffix)

    # Top-N groups CSV path (CLI > config > default)
    if _cli_has_option("--topn-groups-csv"):
        effective_top_n_groups_csv = args.topn_groups_csv
    elif "topn_groups_csv" in cfg:
        effective_top_n_groups_csv = cfg.get("topn_groups_csv", "")
    else:
        suffix = "_top-n-groups_all.csv" if effective_all_files else "_top-n-groups_common.csv"
        effective_top_n_groups_csv = os.path.join(effective_out_dir, effective_output_prefix + suffix)

    # Top-N files CSV path (CLI > config > default)
    if _cli_has_option("--topn-files-csv"):
        effective_top_n_files_csv = args.topn_files_csv
    elif "topn_files_csv" in cfg:
        effective_top_n_files_csv = cfg.get("topn_files_csv", "")
    else:
        effective_top_n_files_csv = os.path.join(effective_out_dir, effective_output_prefix + "_top-n-files.csv")

    resolve_groups = build_group_resolver(group_defs)

    old_rows = read_elf_csv(args.old_csv)
    new_rows = read_elf_csv(args.new_csv)

    old_map = aggregate_by_file_and_group(old_rows, resolve_groups)
    new_map = aggregate_by_file_and_group(new_rows, resolve_groups)

    keys = compute_joined_keys(old_map, new_map, effective_all_files)
    if not keys:
        raise SystemExit("No files to compare (check --all-files and input CSVs).")

    ordered_groups = write_diff_csv(effective_files_csv, keys, old_map, new_map,
                                    effective_files_sort_by, effective_files_sort_metric)

    print(f"[OK] Wrote diff to: {effective_files_csv}  (files compared: {len(keys)}, groups: {len(ordered_groups)})")

    # Top files list for downstream tools
    topfiles_used_path = os.path.join(effective_out_dir, effective_output_prefix + "_topfiles_used.txt")

    if effective_top_n_groups_csv or effective_top_n_files_csv:
        top_groups, top_files_by_group, total_old, total_new = compute_topn(
            ordered_groups, keys, old_map, new_map,
            effective_top_n_groups, effective_top_n_files,
            effective_group_sort_by, effective_group_sort_metric)

        if effective_top_n_groups_csv:
            write_topn_groups_csv(effective_top_n_groups_csv, top_groups, total_old, total_new)
            print(f"[OK] Wrote Top-N groups (groups: {effective_top_n_groups or 'ALL'}) to: {effective_top_n_groups_csv}")

        if effective_top_n_files_csv:
            write_topn_files_csv(effective_top_n_files_csv, top_groups, top_files_by_group, old_map, new_map)
            print(f"[OK] Wrote Top-N files per group (files/group: {effective_top_n_files or 'ALL'}) to: {effective_top_n_files_csv}")

        # Also write a simple list of used top files (relative_dir/filename) for downstream tools
        write_topfiles_used(topfiles_used_path, top_groups, top_files_by_group)
        print(f"[OK] Wrote Top-N files list to: {topfiles_used_path}")

        # Print Markdown table for Top-N groups with human-readable numbers (K/M, 1024-based)
        print("\n## Top-N Groups (human-readable)\n")
        print("| Group | Total Old | Total New | Total Diff | Diff% |")
        print("|---|---:|---:|---:|---:|")
        for g, gdiff, _absd in top_groups:
            told = total_old.get(g, 0)
            tnew = total_new.get(g, 0)
            pct = safe_diff_pct(told, tnew)
            print(f"| {g} | {humanize_1024(told)} | {humanize_1024(tnew)} | {humanize_1024(gdiff)} | {format_float(pct)}% |")


if __name__ == "__main__":
    main()
