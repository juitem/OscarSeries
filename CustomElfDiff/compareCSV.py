#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Compare two ELF section CSVs (generated by elfinfo.py) and produce a wide diff CSV.

Input CSV expected columns:
  - relative_path
  - filename
  - section_name
  - section_size (integer)

Rules:
- Group name defaults to section_name with the first leading '.' removed and uppercased.
- Also supports predefined "combined" groups (wildcards supported via fnmatch).
- If --all-files is omitted, only files present in BOTH old and new sets are compared.
- Otherwise, include Added (only in new) and Removed (only in old) with zeros on the missing side.

Output CSV columns:
  relative_dir, filename, status,
  <GROUP>_old, <GROUP>_new, <GROUP>_diff, <GROUP>_diff%, ... for all groups detected.
Rows are sorted by |<files-sort-by>_diff| descending (default FILESIZE).

-Top-N:
- Compute total diff per group across all files; pick Top-N groups by |diff| (`--top-n-groups`).
- For each Top group, list Top-N files by |diff| (`--top-n-files`).
- Deprecated: `--top-n` applies to both when specific values are not provided.
- Save Top-N to two files: top-n groups and top-n files (see --topn-groups-csv and --topn-files-csv).
- Top-N groups ordering can be controlled via --group-sort-by and --group-sort-metric.


Notes on diff%:
- diff% = (new - old) / old * 100
- If old == 0:
    - If new == 0 -> 0.0
    - Else -> sign(new-old) * 100.0 (finite cap, not inf), to keep it readable

Settings in --config-file (JSON) are optional and merged; CLI flags explicitly provided take precedence.

All code and comments are in English by request.
"""

import argparse
import csv
import json
import os
import sys
from collections import defaultdict, Counter
from fnmatch import fnmatch
from typing import Dict, List, Tuple, Set

# Detect whether a specific CLI option was explicitly provided
def _cli_has_option(name: str) -> bool:
    # Matches forms like: --opt, --opt=value, --opt value
    argv = sys.argv[1:]
    if any(a == name or a.startswith(name + "=") for a in argv):
        return True
    # Handle "--opt value" form
    for i, a in enumerate(argv[:-1]):
        if a == name:
            return True
    return False

def _target_label_and_suffix(mode: str) -> Tuple[str, str]:
    m = (mode or "common").lower()
    if m == "all":
        return ("ALL", "all")
    elif m in ("added+removed", "added-removed", "added_removed"):
        return ("ADDED+REMOVED", "added-removed")
    else:
        return ("COMMON", "common")

# Load optional config JSON. Returns a dict; missing keys are fine.
def load_config(path: str) -> Dict:
    if not path:
        return {}
    if not os.path.isfile(path):
        return {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            cfg = json.load(f)
    except json.JSONDecodeError as e:
        print(f"[ERROR] Failed to parse config file '{path}': {e}")
        sys.exit(1)
    except Exception as e:
        print(f"[ERROR] Could not load config file '{path}': {e}")
        sys.exit(1)
    if not isinstance(cfg, dict):
        print(f"[ERROR] Config file '{path}' must contain a JSON object at the top level.")
        sys.exit(1)
    return cfg

# ---------- Default Combined Group Definitions ----------
# Wildcards are supported (fnmatch). Keys are group names (UPPERCASE).
# FILESIZE is included by default per earlier spec (section_name == "FILESIZE").
DEFAULT_GROUP_DEFS: Dict[str, List[str]] = {
    "FILESIZE": ["FILESIZE"],
}

# Accept either 'relative_path' (old) or 'base_rel_dir' (new from elfinfo.py)
REQUIRED_ANY_REL = {"relative_path", "base_rel_dir"}
REQUIRED_COLUMNS_OTHERS = {"filename", "section_name", "section_size"}


def parse_args():
    p = argparse.ArgumentParser(description="Compare two ELF section CSVs and output a diff CSV.")
    p.add_argument("--old-csv", required=True, help="Path to old CSV (from elfinfo.py)")
    p.add_argument("--new-csv", required=True, help="Path to new CSV (from elfinfo.py)")
    p.add_argument("--whole-filedata-csv", required=False, default="", help="Path to write the full per-file diff CSV (optional, default: '<prefix>_files_<target>.csv')")
    p.add_argument("--gen-whole-filedata-csv", default="false",
                   choices=["true","false"], help="Whether to generate the whole-filedata CSV. Default: false")
    # Deprecated alias for backward-compatibility
    p.add_argument("--files-csv", required=False, default="",
                   help="[Deprecated] Use --whole-filedata-csv instead.")
    p.add_argument("--out-dir", default="", help="Directory to write output files; defaults to '<oldbasename>_vs_<newbasename>'")
    p.add_argument("--target-files", default="common",
                choices=["all", "common", "added+removed"],
                help="Which files to include: 'all' (common + added + removed), 'common' (intersection only), or 'added+removed' (symmetric difference only). Default: common")
    # Deprecated flag kept for backward-compatibility:
    p.add_argument("--all-files", action="store_true",
                help="[Deprecated] Use --target-files instead. Equivalent to --target-files=all when provided.")    # New options replacing --top-n
    p.add_argument("--top-n-groups", type=int, default=0,
                   help="If >0, include Top N groups by |diff| in the summary")
    p.add_argument("--top-n-files", type=int, default=0,
                   help="If >0, for each Top group include Top N files by |diff| in the summary")
    # Backward-compat (deprecated): if provided, it will set both groups/files unless those are explicitly set
    p.add_argument("--top-n", type=int, default=None,
                   help="[Deprecated] If provided, applies to both --top-n-groups and --top-n-files unless those are explicitly set")
    p.add_argument("--topn-groups-csv", default="",
                   help="CSV to store Top-N groups only (no per-file rows)")
    p.add_argument("--topn-files-csv", default="",
                   help="CSV to store Top-N files per group (includes per-file rows with their group)")
    # Deprecated:
    p.add_argument("--groups-csv", default="",
                   help="Optional CSV to store the Top-N groups summary")
    p.add_argument("--config-file", default="",
                   help="Optional JSON file to override/add combined group definitions")
    p.add_argument("--files-sort-by", default="FILESIZE",
                   help="For files CSV: group name to sort rows by |diff| descending (default: FILESIZE)")
    p.add_argument("--files-sort-metric", default="absdiff",
                   choices=["absdiff", "diff", "old", "new", "diff%"],
                   help="For files CSV: metric used with --files-sort-by (default: absdiff)")
    p.add_argument("--group-sort-by", default="metric", choices=["metric", "name"],
                help="How to sort groups in Top-N outputs: 'metric' (by chosen metric) or 'name' (alphabetical). Default: metric")
    p.add_argument("--group-sort-metric", default="absdiff",
                choices=["absdiff", "diff", "old", "new", "diff%"],
                help="Metric used when --group-sort-by=metric for Top-N groups (default: absdiff)")

    p.add_argument("--output-prefix", default="", help="Prefix for output files; if not provided, defaults to '<oldbasename>_vs_<newbasename>'")
    return p.parse_args()


def load_group_defs(path: str) -> Dict[str, List[str]]:
    # Backward compatibility: keep reading only groups from the JSON file if provided.
    if path and os.path.isfile(path):
        with open(path, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except Exception:
                data = {}
        user_defs = data.get("groups", data) if isinstance(data, dict) else {}
        merged = {k.upper(): (v if isinstance(v, list) else [v]) for k, v in DEFAULT_GROUP_DEFS.items()}
        if isinstance(user_defs, dict):
            for k, v in user_defs.items():
                merged[k.upper()] = v if isinstance(v, list) else [v]
        return merged
    return {k.upper(): v[:] for k, v in DEFAULT_GROUP_DEFS.items()}


def read_elf_csv(path: str) -> List[Dict]:
    rows: List[Dict] = []
    with open(path, "r", encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        # Validate required columns flexibly: allow either 'relative_path' or 'base_rel_dir'
        fieldnames = set(reader.fieldnames or [])
        if not (REQUIRED_ANY_REL & fieldnames):
            raise ValueError(f"{path} is missing a relative path column; expected one of: {sorted(REQUIRED_ANY_REL)}")
        missing = REQUIRED_COLUMNS_OTHERS - fieldnames
        if missing:
            raise ValueError(f"{path} is missing required columns: {sorted(missing)}")
        for r in reader:
            try:
                size = int(r["section_size"]) if r["section_size"] != "" else 0
            except Exception:
                size = 0
            rel = r.get("relative_path", "")
            if rel == "":
                rel = r.get("base_rel_dir", "")
            rows.append({
                "relative_path": rel,
                "filename": r["filename"],
                "section_name": r["section_name"],
                "section_size": size,
                # Optional extended fields (may be absent in older CSVs)
                "section_type": r.get("section_type", ""),
                "section_flags_perms": r.get("section_flags_perms", "").replace("|", ""),
                "is_nobits": r.get("is_nobits", ""),
                "in_load_segment": r.get("in_load_segment", ""),
                "load_segment_rwx": r.get("load_segment_rwx", "").replace("|", ""),
                "addr_space": r.get("addr_space", ""),
            })
    return rows


def default_group_name(section_name: str) -> str:
    # Derive group from section name: remove first leading '.' and uppercase the rest.
    # Examples:
    #   ".text" -> "TEXT"
    #   ".rodata.str1.1" -> "RODATA.STR1.1"
    #   "FILESIZE" -> "FILESIZE"
    if not section_name:
        return "NULL"
    if section_name == "FILESIZE":
        return "FILESIZE"
    s = section_name
    if s.startswith("."):
        s = s[1:]
    return s.upper() if s else "NULL"


def build_group_resolver(group_defs: Dict[str, List[str]], rules: List[Dict] = None):
    """
    Return a function that maps (section_name) -> set of group names it belongs to.
    A section can belong to multiple combined groups; also include its default group.
    """
    patterns_by_group = {g.upper(): [p for p in patterns] for g, patterns in group_defs.items()}
    # Normalize rules from JSON (list of {"if": {...}, "group": "NAME"})
    rules = rules or []

    def _as_list(x):
        if x is None:
            return []
        return x if isinstance(x, list) else [x]

    def _rule_matches(rule_if: Dict[str, str], section_name: str, row: Dict[str, str]) -> bool:
        # All conditions in "if" must match (AND)
        for key, pat in (rule_if or {}).items():
            # Value to test
            if key == "section_name":
                val = section_name or ""
            else:
                val = str(row.get(key, "") or "")
            # Patterns can be string or list; use fnmatch for flexibility
            pats = _as_list(pat)
            if not pats:
                return False
            matched_any = False
            for p in pats:
                p = str(p)
                # Use fnmatch for all string fields; exact match also covered by fnmatch
                if fnmatch(val, p):
                    matched_any = True
                    break
            if not matched_any:
                return False
        return True

    def resolve(section_name: str, row: Dict[str, str]) -> Set[str]:
        # 1) Name-based first-match rules
        out: Set[str] = set()
        matched = None
        for g, patterns in patterns_by_group.items():
            for pat in patterns:
                if fnmatch(section_name, pat):
                    matched = g
                    break
            if matched:
                break
        if matched:
            out.add(matched)
            return out

        # 2) JSON-configured attribute/name/filename-based rules (first match wins)
        for rule in rules:
            target = (rule or {}).get("group", "")
            cond = (rule or {}).get("if", {})
            if target and isinstance(cond, dict) and _rule_matches(cond, section_name, row):
                return {str(target).upper()}

        # 3) Default group fallback
        return {default_group_name(section_name)}

    return resolve


# Collect actual section names observed per group from a list of rows
# rows: list of dicts with keys: relative_path, filename, section_name, section_size
# resolve_groups: function(section_name) -> set of group names
# Returns: Dict[group_name] -> Set[section_name]
from typing import Set
def collect_group_sections(rows: List[Dict], resolve_groups) -> Dict[str, Set[str]]:
    mapping: Dict[str, Set[str]] = defaultdict(set)
    for r in rows:
        sec = r.get("section_name", "")
        for g in resolve_groups(sec, r):
            mapping[g].add(sec)
    return mapping


# Aggregates occurrence counts for optional section attributes observed in rows, per group.
def collect_group_section_attrs(rows: List[Dict], resolve_groups) -> Dict[str, Dict[str, Counter]]:
    """
    Returns: Dict[group] -> Dict[attr_name] -> Counter[str -> int]
    Aggregates occurrence counts for optional section attributes observed in rows.
    Attributes counted:
      - section_type
      - section_flags_perms
      - is_nobits
      - load_segment_rwx
      - addr_space
    """
    attrs_map: Dict[str, Dict[str, Counter]] = defaultdict(lambda: defaultdict(Counter))
    for r in rows:
        sec = r.get("section_name", "")
        for g in resolve_groups(sec, r):
            if "section_type" in r:
                v = str(r.get("section_type", "")).strip()
                if v != "":
                    attrs_map[g]["section_type"][v] += 1
            if "section_flags_perms" in r:
                v = str(r.get("section_flags_perms", "")).strip()
                if v != "":
                    attrs_map[g]["section_flags_perms"][v] += 1
            if "is_nobits" in r:
                v = str(r.get("is_nobits", "")).strip()
                if v != "":
                    attrs_map[g]["is_nobits"][v] += 1
            if "in_load_segment" in r:
                v = str(r.get("in_load_segment", "")).strip()
                if v != "":
                    attrs_map[g]["in_load_segment"][v] += 1
            if "load_segment_rwx" in r:
                v = str(r.get("load_segment_rwx", "")).strip()
                if v != "":
                    attrs_map[g]["load_segment_rwx"][v] += 1
            if "addr_space" in r:
                v = str(r.get("addr_space", "")).strip()
                if v != "":
                    attrs_map[g]["addr_space"][v] += 1
    return attrs_map


def aggregate_by_file_and_group(rows: List[Dict], resolve_groups) -> Dict[Tuple[str, str], Dict[str, int]]:
    """
    Return: {(relative_path, filename): {group: total_size}}
    """
    agg: Dict[Tuple[str, str], Dict[str, int]] = defaultdict(lambda: defaultdict(int))
    for r in rows:
        key = (r["relative_path"], r["filename"])
        secname = r["section_name"]
        size = r["section_size"]
        groups = resolve_groups(secname, r)
        for g in groups:
            agg[key][g] += size
    return agg


def safe_diff_pct(old: int, new: int) -> float:
    diff = new - old
    if old == 0:
        if new == 0:
            return 0.0
        return 100.0 if diff > 0 else -100.0
    return (diff / old) * 100.0


def format_float(x: float) -> str:
    return f"{x:.2f}"

# Human-readable 1024-based formatter (e.g., 1536 -> "1.50K", 1048576 -> "1.00M")
def humanize_1024(n: int) -> str:
    sign = "-" if n < 0 else ""
    a = abs(n)
    if a >= 1024 * 1024:
        return f"{sign}{a / (1024*1024):.2f}M"
    if a >= 1024:
        return f"{sign}{a / 1024:.2f}K"
    return f"{n}"

def compute_joined_keys(old_map: Dict, new_map: Dict, target_mode: str) -> List[Tuple[str, str]]:
    old_keys = set(old_map.keys())
    new_keys = set(new_map.keys())
    mode = (target_mode or "common").lower()
    if mode == "all":
        keys = sorted(old_keys | new_keys)
    elif mode in ("added+removed", "added-removed", "added_removed"):
        keys = sorted((old_keys | new_keys) - (old_keys & new_keys))  # symmetric difference only
    else:
        keys = sorted(old_keys & new_keys)
    return keys


def write_diff_csv(path: str,
                   keys: List[Tuple[str, str]],
                   old_map: Dict[Tuple[str, str], Dict[str, int]],
                   new_map: Dict[Tuple[str, str], Dict[str, int]],
                   sort_by_group: str = "FILESIZE",
                   sort_metric: str = "absdiff") -> List[str]:
    """
    Write the wide diff CSV and return the ordered group list used in the header.
    The third column is `status` (Common/Added/Removed).
    """
    # Determine all groups across compared keys (union)
    seen = set()
    for k in keys:
        for src in (old_map.get(k, {}), new_map.get(k, {})):
            for g in src.keys():
                seen.add(g)

    ordered_groups: List[str] = []
    if "FILESIZE" in seen:
        ordered_groups.append("FILESIZE")
    ordered_groups.extend(sorted([g for g in seen if g != "FILESIZE"]))

    # Header with status as 3rd column
    header = ["relative_dir", "filename", "status"]
    for g in ordered_groups:
        header.extend([f"{g}_old", f"{g}_new", f"{g}_diff", f"{g}_diff%"])

    # Sort keys by metric for the requested group (default FILESIZE), descending
    sort_group = (sort_by_group or "FILESIZE").upper()
    metric = (sort_metric or "absdiff").lower()
    if sort_group in ordered_groups:
        def _metric_value(k: Tuple[str, str]):
            om = old_map.get(k, {})
            nm = new_map.get(k, {})
            oldv = om.get(sort_group, 0)
            newv = nm.get(sort_group, 0)
            diff = newv - oldv
            if metric == "diff":
                return diff
            elif metric == "absdiff":
                return abs(diff)
            elif metric == "old":
                return oldv
            elif metric == "new":
                return newv
            elif metric == "diff%":
                return safe_diff_pct(oldv, newv)
            return abs(diff)
        keys = sorted(keys, key=lambda k: (_metric_value(k), k[0], k[1]), reverse=True)
    # else: keep original order

    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(header)
        for rel, name in keys:
            om = old_map.get((rel, name), {})
            nm = new_map.get((rel, name), {})

            if om and nm:
                status = "Common"
            elif om and not nm:
                status = "Removed"
            else:
                status = "Added"

            # rel is the relative directory, now mapped to "relative_dir" column.
            row = [rel, name, status]
            for g in ordered_groups:
                oldv = om.get(g, 0)
                newv = nm.get(g, 0)
                diff = newv - oldv
                pct = safe_diff_pct(oldv, newv)
                row.extend([oldv, newv, diff, format_float(pct)])
            w.writerow(row)

    return ordered_groups


def compute_topn(ordered_groups: List[str],
                 keys: List[Tuple[str, str]],
                 old_map: Dict[Tuple[str, str], Dict[str, int]],
                 new_map: Dict[Tuple[str, str], Dict[str, int]],
                 n_groups: int,
                 n_files: int,
                 group_sort_by: str = "metric",
                 group_sort_metric: str = "absdiff"):
    """
    Returns:
      top_groups: list[(group, total_diff, total_abs_diff)]  # length <= n_groups
      top_files_by_group: dict[group] -> list[(key, old, new, diff, diff%)]  # each length <= n_files
      total_old: dict[group] -> int, total_new: dict[group] -> int
    """
    total_old: Dict[str, int] = defaultdict(int)
    total_new: Dict[str, int] = defaultdict(int)
    totals_diff: Dict[str, int] = defaultdict(int)
    for k in keys:
        om = old_map.get(k, {})
        nm = new_map.get(k, {})
        for g in ordered_groups:
            o = om.get(g, 0)
            n = nm.get(g, 0)
            total_old[g] += o
            total_new[g] += n
            totals_diff[g] += (n - o)

    # Build list of groups and sort per settings
    def _group_metric_value(g: str) -> float:
        m = (group_sort_metric or "absdiff").lower()
        if m == "diff":
            return float(totals_diff[g])
        elif m == "absdiff":
            return float(abs(totals_diff[g]))
        elif m == "old":
            return float(total_old[g])
        elif m == "new":
            return float(total_new[g])
        elif m == "diff%":
            return safe_diff_pct(total_old[g], total_new[g])
        return float(abs(totals_diff[g]))

    groups_list = list(ordered_groups)
    if group_sort_by == "name":
        groups_list.sort(key=lambda x: x)
    else:
        groups_list.sort(key=lambda x: (_group_metric_value(x), x), reverse=True)

    if n_groups:
        groups_list = groups_list[:n_groups]

    top_groups = [(g, totals_diff[g], abs(totals_diff[g])) for g in groups_list]

    top_files_by_group: Dict[str, List] = {}
    limit_files = n_files if n_files else None
    for g, diff, _ in top_groups:
        per_file = []
        for k in keys:
            rel, name = k
            om = old_map.get(k, {})
            nm = new_map.get(k, {})
            oldv = om.get(g, 0)
            newv = nm.get(g, 0)
            d = newv - oldv
            if d != 0:
                per_file.append((k, oldv, newv, d, safe_diff_pct(oldv, newv)))
        per_file_sorted = sorted(per_file, key=lambda t: abs(t[3]), reverse=True)
        if limit_files is not None:
            per_file_sorted = per_file_sorted[:limit_files]
        top_files_by_group[g] = per_file_sorted

    return top_groups, top_files_by_group, total_old, total_new



# --- Write Top-N Groups CSV ---
def write_topn_groups_csv(path: str,
                          top_groups,
                          total_old: Dict[str, int],
                          total_new: Dict[str, int]):
    """
    Columns: group, total_old, total_new, total_diff, total_diff%
    """
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["group", "total_old", "total_new", "total_diff", "total_diff%"])
        for g, gdiff, _ in top_groups:
            told = total_old.get(g, 0)
            tnew = total_new.get(g, 0)
            pct = safe_diff_pct(told, tnew)
            w.writerow([g, told, tnew, gdiff, format_float(pct)])


def write_topn_files_csv(path: str,
                          top_groups,
                          top_files_by_group,
                          old_map,
                          new_map):
    """
    Columns: group, file_relative_dir, file_name, status, old, new, diff, diff%
    """
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["group", "file_relative_dir", "file_name", "status", "old", "new", "diff", "diff%"])
        for g, _gdiff, _abs in top_groups:
            files = top_files_by_group.get(g, [])
            for (rel, name), oldv, newv, d, pct in files:
                omf = old_map.get((rel, name), {})
                nmf = new_map.get((rel, name), {})
                if omf and nmf:
                    status = "Common"
                elif omf and not nmf:
                    status = "Removed"
                else:
                    status = "Added"
                w.writerow([g, rel, name, status, oldv, newv, d, format_float(pct)])


# Write a full groups report in Markdown: Group | Sections | Total Old | Total New | Total Diff | Diff%
# groups_order: list of group names in desired order
# total_old/new: dicts with totals for all groups
# group_sections: dict[group] -> set(section_names)
def write_groups_report_md(path: str,
                           groups_order: List[str],
                           total_old: Dict[str, int],
                           total_new: Dict[str, int],
                           group_sections: Dict[str, Set[str]],
                           group_attrs: Dict[str, Dict[str, Counter]],
                           target_label: str,
                           cmdline: str):
    """
    Write a full groups report in Markdown:
    Columns:
      Group | Sections | Total Old | Total New | Total Diff | Diff% | section_type | section_flags_perms | is_nobits | in_load_segment | load_segment_rwx | addr_space
    Extended attribute columns are included but left empty if not available.
    """
    with open(path, "w", encoding="utf-8") as f:
        # Header
        f.write(f"## Groups Report (Custom-Group, target: {target_label})\n\n")
        f.write("```bash\n")
        f.write(f"$ {cmdline}\n")
        f.write("```\n\n")
        f.write("| Group | Sections | Total Old | Total New | Total Diff | Diff% | section type | section flags/perms | is nobits | in load segment | load segment attr | addr space |\n")
        f.write("|---|---|---:|---:|---:|---:|---|---|---|---|---|---|\n")

        # FILESIZE first if present, then the rest in order
        ordered = list(groups_order)
        if "FILESIZE" in ordered:
            ordered = ["FILESIZE"] + [g for g in ordered if g != "FILESIZE"]

        for g in ordered:
            told = total_old.get(g, 0)
            tnew = total_new.get(g, 0)
            diff = tnew - told
            pct = safe_diff_pct(told, tnew)
            secs = sorted(s for s in group_sections.get(g, set()))
            secs_joined = " ".join(s.replace("|", "") for s in secs)

            # Extended attributes per group (join unique values with spaces, stable order)
            ga = group_attrs.get(g, {})
            def _join(attr: str) -> str:
                ctr = ga.get(attr, Counter())
                # Sort by count desc, then by key asc for stability
                items = sorted(ctr.items(), key=lambda kv: (-kv[1], kv[0]))
                return " ".join(f"{k.replace('|', '')}({v})" for k, v in items)

            f.write(
                f"| {g} | {secs_joined} | {humanize_1024(told)} | {humanize_1024(tnew)} | {humanize_1024(diff)} | {format_float(pct)}% | "
                f"{_join('section_type')} | {_join('section_flags_perms')} | {_join('is_nobits')} | {_join('in_load_segment')} | {_join('load_segment_rwx')} | {_join('addr_space')} |\n"
            )


def write_topfiles_used(path: str,
                        top_groups,
                        top_files_by_group):
    seen = set()
    lines = []
    for g, _gdiff, _abs in top_groups:
        for (rel, name), _oldv, _newv, _d, _pct in top_files_by_group.get(g, []):
            key = (rel, name)
            if key in seen:
                continue
            seen.add(key)
            if rel:
                lines.append(f"{rel}/{name}")
            else:
                lines.append(name)
    with open(path, "w", encoding="utf-8") as f:
        for line in lines:
            f.write(line + "\n")


def _as_bool_str(x) -> bool:
    return str(x).strip().lower() in ("1", "true", "yes", "y", "on")


def collect_ordered_groups(keys: List[Tuple[str, str]],
                           old_map: Dict[Tuple[str, str], Dict[str, int]],
                           new_map: Dict[Tuple[str, str], Dict[str, int]]) -> List[str]:
    seen = set()
    for k in keys:
        for src in (old_map.get(k, {}), new_map.get(k, {})):
            for g in src.keys():
                seen.add(g)
    ordered = []
    if "FILESIZE" in seen:
        ordered.append("FILESIZE")
    ordered.extend(sorted([g for g in seen if g != "FILESIZE"]))
    return ordered


def main():
    args = parse_args()
    cfg = load_config(args.config_file)

    # Deprecated flag handling
    if getattr(args, "groups_csv", ""):
        print("[WARN] --groups-csv is deprecated and ignored. Use --topn-groups-csv and --topn-files-csv instead.")

    # Groups: prefer config['groups'] if available, else fall back to file-as-groups or defaults
    if isinstance(cfg.get("groups"), dict):
        # Merge defaults with config groups
        group_defs = {k.upper(): v[:] for k, v in DEFAULT_GROUP_DEFS.items()}
        for k, v in cfg["groups"].items():
            group_defs[k.upper()] = v if isinstance(v, list) else [v]
    else:
        group_defs = load_group_defs(args.config_file)

    # Compute effective output prefix
    if args.output_prefix:
        effective_output_prefix = args.output_prefix
    else:
        oldb = os.path.splitext(os.path.basename(args.old_csv))[0]
        newb = os.path.splitext(os.path.basename(args.new_csv))[0]
        effective_output_prefix = f"{oldb}_vs_{newb}"

    # Compute effective output directory
    if args.out_dir:
        effective_out_dir = args.out_dir
    else:
        effective_out_dir = effective_output_prefix
    os.makedirs(effective_out_dir, exist_ok=True)

    # Determine target files mode (CLI > config > defaults). Support deprecated --all-files.
    if _cli_has_option("--target-files"):
        effective_target_files = args.target_files
    elif _cli_has_option("--all-files") and args.all_files:
        effective_target_files = "all"
    else:
        # config: prefer new key 'target_files'; fall back to legacy 'all_files'
        if "target_files" in cfg:
            effective_target_files = str(cfg.get("target_files", "common")).lower()
        elif cfg.get("all_files", False):
            effective_target_files = "all"
        else:
            effective_target_files = "common"
    target_label, target_suffix = _target_label_and_suffix(effective_target_files)

    # Determine Top-N (groups/files) with CLI precedence and legacy support
    # CLI explicit checks
    cli_has_tng = _cli_has_option("--top-n-groups")
    cli_has_tnf = _cli_has_option("--top-n-files")
    cli_has_tn  = _cli_has_option("--top-n")

    # Base values from config (prefer new keys; fall back to legacy 'top_n')
    cfg_tng = cfg.get("top_n_groups")
    cfg_tnf = cfg.get("top_n_files")
    cfg_tn  = cfg.get("top_n")

    # Start from argparse defaults
    effective_top_n_groups = args.top_n_groups
    effective_top_n_files  = args.top_n_files

    if cli_has_tng:
        effective_top_n_groups = args.top_n_groups
    elif cfg_tng is not None:
        effective_top_n_groups = int(cfg_tng)
    elif cli_has_tn and args.top_n is not None:
        effective_top_n_groups = int(args.top_n)
    elif cfg_tn is not None:
        effective_top_n_groups = int(cfg_tn)

    if cli_has_tnf:
        effective_top_n_files = args.top_n_files
    elif cfg_tnf is not None:
        effective_top_n_files = int(cfg_tnf)
    elif cli_has_tn and args.top_n is not None:
        effective_top_n_files = int(args.top_n)
    elif cfg_tn is not None:
        effective_top_n_files = int(cfg_tn)


    # Files sorting options (CLI > config > defaults)
    if _cli_has_option("--files-sort-by"):
        effective_files_sort_by = args.files_sort_by
    else:
        effective_files_sort_by = cfg.get("files_sort_by", args.files_sort_by)

    if _cli_has_option("--files-sort-metric"):
        effective_files_sort_metric = args.files_sort_metric
    else:
        effective_files_sort_metric = cfg.get("files_sort_metric", args.files_sort_metric)

    # Group sorting options for Top-N outputs
    if _cli_has_option("--group-sort-by"):
        effective_group_sort_by = args.group_sort_by
    else:
        effective_group_sort_by = cfg.get("group_sort_by", args.group_sort_by)

    if _cli_has_option("--group-sort-metric"):
        effective_group_sort_metric = args.group_sort_metric
    else:
        effective_group_sort_metric = cfg.get("group_sort_metric", args.group_sort_metric)

    # Determine whole-filedata CSV output path (CLI > config > default), with deprecated alias support
    if _cli_has_option("--whole-filedata-csv"):
        effective_whole_filedata_csv = args.whole_filedata_csv
    elif _cli_has_option("--files-csv") and args.files_csv:
        print("[WARN] --files-csv is deprecated. Use --whole-filedata-csv instead.")
        effective_whole_filedata_csv = args.files_csv
    elif "whole_filedata_csv" in cfg:
        effective_whole_filedata_csv = cfg.get("whole_filedata_csv", "")
    elif "files_csv" in cfg:
        # legacy config key
        effective_whole_filedata_csv = cfg.get("files_csv", "")
    else:
        suffix = f"_files_{target_suffix}.csv"
        effective_whole_filedata_csv = os.path.join(effective_out_dir, effective_output_prefix + suffix)
    # Determine whether to generate the whole-filedata CSV
    if _cli_has_option("--gen-whole-filedata-csv"):
        gen_whole_filedata_csv = _as_bool_str(args.gen_whole_filedata_csv)
    else:
        # from config (bool or string), default false
        cfg_gen = cfg.get("gen_whole_filedata_csv", "false")
        gen_whole_filedata_csv = _as_bool_str(cfg_gen)

    # Top-N groups CSV path (CLI > config > default)
    if _cli_has_option("--topn-groups-csv"):
        effective_top_n_groups_csv = args.topn_groups_csv
    elif "topn_groups_csv" in cfg:
        effective_top_n_groups_csv = cfg.get("topn_groups_csv", "")
    else:
        suffix = f"_top-n-groups_{target_suffix}.csv"
        effective_top_n_groups_csv = os.path.join(effective_out_dir, effective_output_prefix + suffix)
    # Top-N files CSV path (CLI > config > default)
    if _cli_has_option("--topn-files-csv"):
        effective_top_n_files_csv = args.topn_files_csv
    elif "topn_files_csv" in cfg:
        effective_top_n_files_csv = cfg.get("topn_files_csv", "")
    else:
        files_suffix = f"_top-n-files_{target_suffix}.csv"
        effective_top_n_files_csv = os.path.join(effective_out_dir, effective_output_prefix + files_suffix)

    resolve_groups = build_group_resolver(group_defs, cfg.get("rules", []))

    old_rows = read_elf_csv(args.old_csv)
    new_rows = read_elf_csv(args.new_csv)

    # Build mapping of group -> actual section names observed (from both old and new)
    group_sections_old = collect_group_sections(old_rows, resolve_groups)
    group_sections_new = collect_group_sections(new_rows, resolve_groups)
    group_sections: Dict[str, Set[str]] = defaultdict(set)
    for g, ss in group_sections_old.items():
        group_sections[g].update(ss)
    for g, ss in group_sections_new.items():
        group_sections[g].update(ss)

    # Build mapping of group -> aggregated extended attributes (from both old and new)
# Build mapping of group -> aggregated extended attributes (from both old and new)
    group_attrs_old = collect_group_section_attrs(old_rows, resolve_groups)
    group_attrs_new = collect_group_section_attrs(new_rows, resolve_groups)
    group_attrs: Dict[str, Dict[str, Counter]] = defaultdict(lambda: defaultdict(Counter))
    for g, amap in group_attrs_old.items():
        for k, ctr in amap.items():
            group_attrs[g][k].update(ctr)
    for g, amap in group_attrs_new.items():
        for k, ctr in amap.items():
            group_attrs[g][k].update(ctr)

    old_map = aggregate_by_file_and_group(old_rows, resolve_groups)
    new_map = aggregate_by_file_and_group(new_rows, resolve_groups)

    keys = compute_joined_keys(old_map, new_map, effective_target_files)
    if not keys:
        raise SystemExit("No files to compare (check --all-files and input CSVs).")

    if gen_whole_filedata_csv:
        ordered_groups = write_diff_csv(effective_whole_filedata_csv, keys, old_map, new_map,
                                        effective_files_sort_by, effective_files_sort_metric)
        print(f"[OK] Wrote diff to: {effective_whole_filedata_csv}  (target={target_label}, files compared: {len(keys)}, groups: {len(ordered_groups)})")
    else:
        ordered_groups = collect_ordered_groups(keys, old_map, new_map)
        print(f"[SKIP] Whole-filedata CSV generation disabled (target={target_label}, files compared: {len(keys)}, groups: {len(ordered_groups)})")

    # Top files list for downstream tools
    topfiles_used_path = os.path.join(
        effective_out_dir,
        effective_output_prefix + f"_topfiles_used_{target_suffix}.txt"
    )

    if effective_top_n_groups_csv or effective_top_n_files_csv:
        top_groups, top_files_by_group, total_old, total_new = compute_topn(
            ordered_groups, keys, old_map, new_map,
            effective_top_n_groups, effective_top_n_files,
            effective_group_sort_by, effective_group_sort_metric)

        if effective_top_n_groups_csv:
            write_topn_groups_csv(effective_top_n_groups_csv, top_groups, total_old, total_new)
            print(f"[OK] Wrote Top-N groups (groups: {effective_top_n_groups or 'ALL'}) to: {effective_top_n_groups_csv}")

        if effective_top_n_files_csv:
            write_topn_files_csv(effective_top_n_files_csv, top_groups, top_files_by_group, old_map, new_map)
            print(f"[OK] Wrote Top-N files per group (files/group: {effective_top_n_files or 'ALL'}) to: {effective_top_n_files_csv}")

        # Also write a simple list of used top files (relative_dir/filename) for downstream tools
        write_topfiles_used(topfiles_used_path, top_groups, top_files_by_group)
        print(f"[OK] Wrote Top-N files list to: {topfiles_used_path}")

        # Print Markdown table for Top-N groups with human-readable numbers (K/M, 1024-based)
        print(f"\n## Top-N Groups (Custom-Group, target: {target_label})\n")
        cmdline = " ".join(sys.argv)
        print("```bash")
        print(f"$ {cmdline}")
        print("```")
        print()
        print("| Group | Total Old | Total New | Total Diff | Diff% |")
        print("|---|---:|---:|---:|---:|")
        # Reorder for Markdown: FILESIZE first (if present), then the rest in the already-selected/sorted order
        md_groups = list(top_groups)
        for i, (g0, _, _) in enumerate(md_groups):
            if g0 == "FILESIZE":
                md_groups.insert(0, md_groups.pop(i))
                break
        for g, gdiff, _absd in md_groups:
            told = total_old.get(g, 0)
            tnew = total_new.get(g, 0)
            pct = safe_diff_pct(told, tnew)
            print(f"| {g} | {humanize_1024(told)} | {humanize_1024(tnew)} | {humanize_1024(gdiff)} | {format_float(pct)}% |")

        # Also save the Markdown summary to a file
        topn_groups_md = os.path.join(
            effective_out_dir,
            effective_output_prefix + f"_top-n-groups_{target_suffix}.md"
        )
        with open(topn_groups_md, "w", encoding="utf-8") as fmd:
            fmd.write(f"## Top-N Groups (Custom-Group, target: {target_label})\n\n")
            fmd.write("```bash\n")
            fmd.write(f"$ {' '.join(sys.argv)}\n")
            fmd.write("```\n\n")
            fmd.write("| Group | Total Old | Total New | Total Diff | Diff% |\n")
            fmd.write("|---|---:|---:|---:|---:|\n")
            md_groups = list(top_groups)
            for i, (g0, _, _) in enumerate(md_groups):
                if g0 == "FILESIZE":
                    md_groups.insert(0, md_groups.pop(i))
                    break
            for g, gdiff, _absd in md_groups:
                told = total_old.get(g, 0)
                tnew = total_new.get(g, 0)
                pct = safe_diff_pct(told, tnew)
                fmd.write(f"| {g} | {humanize_1024(told)} | {humanize_1024(tnew)} | {humanize_1024(gdiff)} | {format_float(pct)}% |\n")
        print(f"[OK] Wrote Top-N groups Markdown to: {topn_groups_md}")

    # Compute totals for ALL groups (n_groups=0) in the selected order for reporting
    all_groups_list, _dummy_top_files, total_old_all, total_new_all = compute_topn(
        ordered_groups, keys, old_map, new_map,
        0, 0,  # no truncation: include all groups/files
        effective_group_sort_by, effective_group_sort_metric)
    # all_groups_list is list of tuples (g, total_diff, abs_diff); extract group order in that sorted sequence
    groups_order_for_report = [g for g, _d, _a in all_groups_list]

    # Write groups report Markdown
    groups_report_md = os.path.join(
        effective_out_dir,
        effective_output_prefix + f"_groups-report_{target_suffix}.md"
    )
    write_groups_report_md(groups_report_md, groups_order_for_report, total_old_all, total_new_all, group_sections, group_attrs, target_label, " ".join(sys.argv))
    print(f"[OK] Wrote Groups report Markdown to: {groups_report_md}")


if __name__ == "__main__":
    main()
